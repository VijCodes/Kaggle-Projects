{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Importing Modules","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import LayerNormalization,Input\n","metadata":{"execution":{"iopub.status.busy":"2023-07-22T05:00:26.013888Z","iopub.execute_input":"2023-07-22T05:00:26.014268Z","iopub.status.idle":"2023-07-22T05:00:26.020163Z","shell.execute_reply.started":"2023-07-22T05:00:26.014236Z","shell.execute_reply":"2023-07-22T05:00:26.018840Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/segment-anything-zip-file/segment-anything\")\nsys.path.append(\"/kaggle/input/smpmod/segmentation_models.pytorch/segmentation_models_pytorch\")\nimport losses as ls","metadata":{"execution":{"iopub.status.busy":"2023-07-22T05:01:39.921902Z","iopub.execute_input":"2023-07-22T05:01:39.922525Z","iopub.status.idle":"2023-07-22T05:01:39.979993Z","shell.execute_reply.started":"2023-07-22T05:01:39.922482Z","shell.execute_reply":"2023-07-22T05:01:39.978924Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import segment_anything as sam","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:32:52.482659Z","iopub.execute_input":"2023-07-21T22:32:52.483268Z","iopub.status.idle":"2023-07-21T22:32:52.731695Z","shell.execute_reply.started":"2023-07-21T22:32:52.483235Z","shell.execute_reply":"2023-07-21T22:32:52.730653Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#validation_meta = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation_metadata.json')\n#train_metadata = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/train_metadata.json')\nsample_submission = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:32:52.734489Z","iopub.execute_input":"2023-07-21T22:32:52.734839Z","iopub.status.idle":"2023-07-21T22:32:52.748678Z","shell.execute_reply.started":"2023-07-21T22:32:52.734806Z","shell.execute_reply":"2023-07-21T22:32:52.747707Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Configuration","metadata":{}},{"cell_type":"code","source":"config = {\n    'batch_size': 16,\n    'learning_rate': 0.001,\n    'n_epochs': 10,\n    'loss_smooth':1.0\n    # Add any other hyperparameters you want to track\n}\n\n# Use the hyperparameters from the config dictionary\nbatch_size = config['batch_size']\nlearning_rate = config['learning_rate']\nn_epochs = config['n_epochs']\nseq_len ,channels ,height ,width =  8 ,3 ,256 ,256","metadata":{"execution":{"iopub.status.busy":"2023-07-22T05:04:49.750175Z","iopub.execute_input":"2023-07-22T05:04:49.750541Z","iopub.status.idle":"2023-07-22T05:04:49.756958Z","shell.execute_reply.started":"2023-07-22T05:04:49.750511Z","shell.execute_reply":"2023-07-22T05:04:49.755944Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"#### Setting up lazy-loading and preprocessing functions","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n_T11_BOUNDS = (243, 303)\n_CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n_TDIFF_BOUNDS = (-4, 2)\n\ndef normalize_range(data, bounds):\n    \"\"\"Maps data to the range [0, 1].\"\"\"\n    return (data - bounds[0]) / (bounds[1] - bounds[0])\n\ndef getdata(obs_path):\n    bands_data = {}\n\n    # Load all band data\n    for filename in os.listdir(obs_path):\n        if \"band\" in filename:\n            band_name = filename.split('.')[0]  # get the name of the band (excluding the .npy extension)\n            file_path = os.path.join(obs_path, filename)  # full path of the file\n            band_data = np.load(file_path)  # load the band data\n            bands_data[band_name] = band_data  # store the band data in the dictionary\n    \n    # Load the aggregated contrail markings as labels\n    label_path = os.path.join(obs_path, 'human_pixel_masks.npy')\n    if os.path.exists(label_path):\n        labels = np.load(label_path)\n    else:\n        labels = None\n    return bands_data, labels\n\ndef get_ash_color_images(bands_data,get_mask_frame_only = False) -> np.array:\n    band11 = bands_data['band_11']\n    band14 = bands_data['band_14']\n    band15 = bands_data['band_15']\n    \n    if get_mask_frame_only:\n        band11 = band11[:,:,4]\n        band14 = band14[:,:,4]\n        band15 = band15[:,:,4]\n\n    r = normalize_range(band15 - band14, _TDIFF_BOUNDS)\n    g = normalize_range(band14 - band11, _CLOUD_TOP_TDIFF_BOUNDS)\n    b = normalize_range(band14, _T11_BOUNDS)\n    false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n    return false_color\n\ndef preprocess_func(bands_data,get_mask_frame_only = False):\n    if get_mask_frame_only:\n        stacked_data = get_ash_color_images(bands_data,get_mask_frame_only = True)\n        stacked_data = stacked_data.transpose(2, 0, 1)\n        return stacked_data\n    else:\n        stacked_data = get_ash_color_images(bands_data,get_mask_frame_only = False)\n        #print(stacked_data.shape) \n        stacked_data = stacked_data.transpose(3, 2, 0,1)\n        return stacked_data","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:32:52.757277Z","iopub.execute_input":"2023-07-21T22:32:52.757640Z","iopub.status.idle":"2023-07-21T22:32:52.772761Z","shell.execute_reply.started":"2023-07-21T22:32:52.757608Z","shell.execute_reply":"2023-07-21T22:32:52.771660Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"4\" face=\"verdana\">\n\n| **Band Number** | **Description** | **Additional Details** |\n| --- | --- | --- |\n| **Band 8** | \"Upper-Level Tropospheric Water Vapor\" Band | This band is helpful in tracking upper-level atmospheric moisture and jet stream winds, aiding in the forecasting of severe weather events and heavy rainfall. |\n| **Band 9** | \"Mid-Level Tropospheric Water Vapor\" Band | This band captures images of mid-tropospheric moisture and atmospheric motion, valuable for identifying features such as tropical cyclones and thunderstorms. |\n| **Band 10** | \"Lower-level Water Vapor\" Band | The lower-level water vapor band assists in identifying low-level moisture content, aiding in the prediction of fog, frost, and low clouds. |\n| **Band 11** | \"Cloud-Top Phase\" Band | This band helps to determine cloud phases (water, mixed, ice) and heights, crucial for aviation safety and general weather prediction. |\n| **Band 12** | \"Ozone Band\" | The ozone band detects ozone concentration in the atmosphere, offering insights into ozone layer health and aiding in the prediction of UV index and air quality. |\n| **Band 13** | \"Clean\" IR Longwave Window Band | Primarily used for detection of clouds at all levels, sea surface temperature, and rainfall. |\n| **Band 14** | IR Longwave Window Band | This band is used for surface and cloud top temperature estimates, identifying cloud types and cloud motion. |\n| **Band 15** | \"Dirty\" Longwave Window Band | Used for estimation of lower-tropospheric water vapor, volcanic ash detection, and for improved rainfall estimation. |\n| **Band 16** | \"CO2\" Longwave Infrared | This band aids in the estimation of cloud height and temperature, especially for high-level clouds. |\n\n</font>","metadata":{}},{"cell_type":"code","source":"class ContrailsData(Dataset):\n    def __init__(self, data_paths, preprocess_func, train_validation):\n        # Initialize your data, download, etc.\n        self.data_paths = data_paths\n        self.preprocess_func = preprocess_func\n        self.train_validation = train_validation\n    \n    def __len__(self):\n        return len(self.data_paths['path'])\n\n    def __getitem__(self, index):\n        obs_path = self.data_paths.iloc[index]['path']\n        bands_data, labels = getdata(obs_path)\n        data = self.preprocess_func(bands_data)\n        labels = labels.transpose(2, 0, 1)\n        labels = torch.from_numpy(labels)\n        labels = labels.float()\n        return data, labels\n\n\ndef get_data_paths(root_dir):\n    all_dirs = []\n    for path in os.listdir(root_dir):\n        full_path = os.path.join(root_dir, path)\n        if os.path.isdir(full_path):\n            all_dirs.append(full_path)\n    return pd.DataFrame(all_dirs, columns=['path'])\n\nroot_dir_train = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/train'\nroot_dir_validation = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation'\n\ntrain_data_paths = get_data_paths(root_dir_train)\nvalidation_data_paths = get_data_paths(root_dir_validation)\n\n# Prepare your data loaders\ntrain_dataset = ContrailsData(train_data_paths, preprocess_func, 'train')\nvalidation_dataset = ContrailsData(validation_data_paths, preprocess_func, 'validation')\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:32:52.774136Z","iopub.execute_input":"2023-07-21T22:32:52.774463Z","iopub.status.idle":"2023-07-21T22:33:06.212760Z","shell.execute_reply.started":"2023-07-21T22:32:52.774432Z","shell.execute_reply":"2023-07-21T22:33:06.211787Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Initialize data and labels\ndata, labels = None, None\n\n# Iterate over the validation data loader\nfor batch_data, batch_labels in validation_dataloader:\n    data, labels = batch_data, batch_labels\n\n# Now data and labels contain the last batch\nprint(f'''data shape: {data.shape}''')  \nprint(f'''labels shape: {labels.shape}''')  ","metadata":{"execution":{"iopub.status.busy":"2023-07-22T01:18:31.972590Z","iopub.execute_input":"2023-07-22T01:18:31.973145Z","iopub.status.idle":"2023-07-22T01:18:32.635679Z","shell.execute_reply.started":"2023-07-22T01:18:31.973108Z","shell.execute_reply":"2023-07-22T01:18:32.634803Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"data shape: torch.Size([16, 8, 3, 256, 256])\nlabels shape: torch.Size([16, 1, 256, 256])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Dice Coefficient","metadata":{}},{"cell_type":"code","source":"class Dice(nn.Module):\n    def __init__(self, use_sigmoid=True):\n        super(Dice, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.use_sigmoid = use_sigmoid\n\n    def forward(self, inputs, targets, smooth=1):\n        if self.use_sigmoid:\n            inputs = self.sigmoid(inputs)       \n        \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return dice\n    \ndice = Dice()\n\nclass DiceThresholdTester:\n    \n    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n        self.model = model\n        self.data_loader = data_loader\n        self.cumulative_mask_pred = []\n        self.cumulative_mask_true = []\n        \n    def precalculate_prediction(self) -> None:\n        sigmoid = nn.Sigmoid()\n        for images, mask_true in self.data_loader:\n            if torch.cuda.is_available():\n                images = images.cuda()\n\n            mask_pred = sigmoid(model.forward(images))\n\n            self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n            self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n            \n        self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n        self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n\n        self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n        self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n    \n    def test_threshold(self, threshold: float) -> float:\n        _dice = Dice(use_sigmoid=False)\n        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n        return _dice(self.cumulative_mask_true, after_threshold).item()\n    \n#dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:06.229956Z","iopub.execute_input":"2023-07-21T22:33:06.230784Z","iopub.status.idle":"2023-07-21T22:33:06.245669Z","shell.execute_reply.started":"2023-07-21T22:33:06.230744Z","shell.execute_reply":"2023-07-21T22:33:06.244603Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Models\n\n*  SimpleConv2D\n*  UNet\n*  Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"class SimpleConv2D(nn.Module):\n    def __init__(self):\n        super(SimpleConv2D, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv_final = nn.Conv2d(256, 1, kernel_size=1, stride=1)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.conv_final(x)\n        \n        # Add a Sigmoid activation function as the last layer\n        #x = torch.sigmoid(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:06.249975Z","iopub.execute_input":"2023-07-21T22:33:06.250451Z","iopub.status.idle":"2023-07-21T22:33:06.264242Z","shell.execute_reply.started":"2023-07-21T22:33:06.250425Z","shell.execute_reply":"2023-07-21T22:33:06.261544Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n        \n        self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        \n        self.conv6 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n        self.conv7 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv8 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv9 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv_final = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self,x):\n        x1 = F.relu(self.conv1(x))\n        x = F.max_pool2d(x1,kernel_size=(2))\n        \n        x2 = F.relu(self.conv2(x))\n        x = F.max_pool2d(x2,kernel_size=(2))\n        \n        x3 = F.relu(self.conv3(x))\n        x = F.max_pool2d(x3,kernel_size=(2))\n        \n        x4 = F.relu(self.conv4(x))\n        x = F.max_pool2d(x4,kernel_size=(2))\n        \n        x5 = F.relu(self.conv5(x))\n        \n        x6 = torch.cat([x4,self.upconv5(x5)],dim=-3)\n        x6 = F.relu(self.conv6(x6))\n        \n        x7 = torch.cat([x3,self.upconv4(x6)],dim=-3)\n        x7 = F.relu(self.conv7(x7))\n        \n        x8 = torch.cat([x2,self.upconv3(x7)],dim=-3)\n        x8 = F.relu(self.conv8(x8))\n        \n        x9 = torch.cat([x1,self.upconv2(x8)],dim=-3)\n        x9 = F.relu(self.conv9(x9))\n        \n        out=self.conv_final(x9)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:06.265944Z","iopub.execute_input":"2023-07-21T22:33:06.267023Z","iopub.status.idle":"2023-07-21T22:33:06.285041Z","shell.execute_reply.started":"2023-07-21T22:33:06.266810Z","shell.execute_reply":"2023-07-21T22:33:06.284013Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn\nimport random\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nbatch_size, seq_len, channels, height, width = 16,8,3,256,256\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        \n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n        self.bn1 = nn.BatchNorm2d(outputChannel)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outputChannel, outputChannel)\n        self.bn2 = nn.BatchNorm2d(outputChannel)\n        self.downsample = downsample\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        caOutput = self.ca(out)\n        out = caOutput * out\n        saOutput = self.sa(out)\n        out = saOutput * out\n        return out, saOutput\n\nclass BasicDownSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        return x\n    \nclass FrameAttention(nn.Module):\n    def __init__(self, heads, key_dim, embed_dim):\n        super().__init__()\n        self.heads = heads\n        self.key_dim = key_dim\n        self.embed_dim = embed_dim\n        self.Linear1 = nn.Linear(self.key_dim, self.embed_dim)\n        self.MultiHead = nn.MultiheadAttention(self.embed_dim, self.heads)\n        self.LayerNorm = nn.LayerNorm(self.embed_dim, eps=1e-6)\n        self.Linear2 = nn.Linear(self.embed_dim, self.key_dim)\n\n    def forward(self, x):\n        y = x[:, 3,:,:,:]\n        x = x.view(x.shape[0], x.shape[1], -1).transpose(0, 1)\n        x = self.Linear1(x)\n        attn_output, _ = self.MultiHead(x, x, x)\n        x = x + attn_output\n        x = self.LayerNorm(x)\n        x = self.Linear2(x)\n        x = x.transpose(0, 1).reshape(batch_size,-1,height ,width)\n        x = torch.cat((y, x), dim=1)\n    \n        return x\n    \n\nclass DownSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass BasicUpSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        return x\n    \nclass UpSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass UNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = BasicDownSample(inputChannel, 32)\n    self.downsample2 = BasicDownSample(32, 64)\n    self.downsample3 = BasicDownSample(64, 128)\n    self.downsample4 = BasicDownSample(128, 256)\n    self.downsample5 = BasicDownSample(256, 512)\n\n    self.upsample1 = BasicUpSample(512, 256)\n    self.upsample2 = BasicUpSample(512, 128)\n    self.upsample3 = BasicUpSample(256, 64)\n    self.upsample4 = BasicUpSample(128, 32)\n    self.upsample5 = BasicUpSample(64, 32)\n    self.classification = self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128 = self.downsample1(x)\n    scale64 = self.downsample2(scale128)\n    scale32 = self.downsample3(scale64)\n    scale16 = self.downsample4(scale32)\n    scale8 = self.downsample5(scale16)\n    upscale16 = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32 = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64 = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128 = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256 = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass AttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128, sa128down = self.downsample1(x)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass ResidualAttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.FrameAttention = FrameAttention(100 ,channels*height*width ,100)\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.residualBlock1 = ResidualBlock(512, 512)\n    self.residualBlock2 = ResidualBlock(512, 512)\n    self.residualBlock3 = ResidualBlock(512, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    FrameAtt = self.FrameAttention(x)\n    scale128, sa128down = self.downsample1(FrameAtt)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    scale8, sa8down = self.residualBlock1(scale8)\n    scale8, sa8down = self.residualBlock2(scale8)\n    scale8, sa8down = self.residualBlock3(scale8)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:49:05.346936Z","iopub.execute_input":"2023-07-21T22:49:05.347317Z","iopub.status.idle":"2023-07-21T22:49:05.406792Z","shell.execute_reply.started":"2023-07-21T22:49:05.347287Z","shell.execute_reply":"2023-07-21T22:49:05.405723Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Building New Layers","metadata":{}},{"cell_type":"markdown","source":"#### Training\n\n*  Had to decrease batch size and empty cache because of memory issues. \n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n# Define the device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize your model\n#model = SimpleConv2D().to(device)\n\n#model  = UNet().to(device)\n\ndata,labels = next(iter(train_dataloader))\n\nmodel  = ResidualAttentionUNet(inputChannel = 27 , outputChannel = 1).to(device)\n\n# Define a list of possible locations where the saved model state might be stored\nlocations = ['ResAttUnet.pth', '/kaggle/input/resattunet-model-state/ResAttUnet.pth']\n\n# Iterate over the list of locations\nfor location in locations:\n    try:\n        # Try to load the model state from the current location\n        model.load_state_dict(torch.load(location))\n        # If loading succeeds, break out of the loop\n        break\n    except: \n        # If loading fails, continue to the next location\n        continue\nelse:\n    # If all locations fail, print an error message\n    print(\"Could not load appropriate model state from any location. Continue with training brand new model\")\n    \n\n# Define loss function\n#criterion = nn.BCEWithLogitsLoss()\n\ncriterion = ls.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# Training loop\nfor epoch in range(n_epochs):\n    model.train()\n    running_loss = 0\n    batches = 0\n    for images, labels in train_dataloader:\n        torch.cuda.empty_cache()\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        #print(images.shape)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        batches+=1\n        #print(f'batch:{batches} completed')\n        if batches%100 == 0: print(f'batches ran: {batches}')\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    print('Train Loss: {:.4f}'.format(epoch_loss))\n    torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T05:04:56.381797Z","iopub.execute_input":"2023-07-22T05:04:56.382523Z","iopub.status.idle":"2023-07-22T06:23:07.359643Z","shell.execute_reply.started":"2023-07-22T05:04:56.382488Z","shell.execute_reply":"2023-07-22T06:23:07.355274Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"batches ran: 100\nbatches ran: 200\nbatches ran: 300\nbatches ran: 400\nbatches ran: 500\nbatches ran: 600\nbatches ran: 700\nbatches ran: 800\nbatches ran: 900\nbatches ran: 1000\nbatches ran: 1100\nbatches ran: 1200\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#print(images.shape)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[15], line 279\u001b[0m, in \u001b[0;36mResidualAttentionUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 279\u001b[0m   FrameAtt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFrameAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m   scale128, sa128down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample1(FrameAtt)\n\u001b[1;32m    281\u001b[0m   scale64, sa64down \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample2(scale128)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[15], line 112\u001b[0m, in \u001b[0;36mFrameAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(x)\n\u001b[1;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLinear2(x)\n\u001b[0;32m--> 112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[16, -1, 256, 256]' is invalid for input of size 1572864"],"ename":"RuntimeError","evalue":"shape '[16, -1, 256, 256]' is invalid for input of size 1572864","output_type":"error"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T04:46:18.625785Z","iopub.execute_input":"2023-07-22T04:46:18.626785Z","iopub.status.idle":"2023-07-22T04:46:19.383272Z","shell.execute_reply.started":"2023-07-22T04:46:18.626753Z","shell.execute_reply":"2023-07-22T04:46:19.381734Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"    # Validation loop\n    model.FrameAttention.batch_size = 16\n    model.eval()\n    print(f\"Evaluating: {epoch}\")\n    with torch.no_grad():\n        running_loss = 0\n        for images, labels in validation_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(validation_dataloader.dataset)\n        print('Validation Loss: {:.4f}'.format(epoch_loss))\n        dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:02:21.250975Z","iopub.execute_input":"2023-07-22T03:02:21.251347Z","iopub.status.idle":"2023-07-22T03:08:44.649959Z","shell.execute_reply.started":"2023-07-22T03:02:21.251318Z","shell.execute_reply":"2023-07-22T03:08:44.647371Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Evaluating: 0\nValidation Loss: 0.0126\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Saving the current state of the model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-22T03:01:54.472258Z","iopub.execute_input":"2023-07-22T03:01:54.472671Z","iopub.status.idle":"2023-07-22T03:01:55.030310Z","shell.execute_reply.started":"2023-07-22T03:01:54.472641Z","shell.execute_reply":"2023-07-22T03:01:55.029339Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    running_loss = 0\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(validation_dataloader.dataset)\n    print('Validation Loss: {:.4f}'.format(epoch_loss))\ndice_threshold_tester.test_threshold(0.03)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:49:08.321574Z","iopub.status.idle":"2023-07-21T22:49:08.322273Z","shell.execute_reply.started":"2023-07-21T22:49:08.322025Z","shell.execute_reply":"2023-07-21T22:49:08.322048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Optimal Dice Coefficient","metadata":{}},{"cell_type":"code","source":"dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)\ndice_threshold_tester.precalculate_prediction()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:19.391224Z","iopub.status.idle":"2023-07-21T22:33:19.392145Z","shell.execute_reply.started":"2023-07-21T22:33:19.391902Z","shell.execute_reply":"2023-07-21T22:33:19.391927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_to_test = [round(x * 0.001, 2) for x in range(200)]\n\noptim_threshold = 0.975\nbest_dice_score = -1\n\nthresholds = []\ndice_scores = []\n\nfor t in thresholds_to_test:\n    dice_score = dice_threshold_tester.test_threshold(t)\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        optim_threshold = t\n    \n    thresholds.append(t)\n    dice_scores.append(dice_score)\n    \nprint(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\ndf_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:19.393335Z","iopub.status.idle":"2023-07-21T22:33:19.394118Z","shell.execute_reply.started":"2023-07-21T22:33:19.393887Z","shell.execute_reply":"2023-07-21T22:33:19.393909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\nplt.axhline(y=best_dice_score, color='green')\nplt.axvline(x=optim_threshold, color='green')\nplt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\nplt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\nplt.ylim(bottom=0)\nplt.title('Threshold vs Dice Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T22:33:19.395414Z","iopub.status.idle":"2023-07-21T22:33:19.396359Z","shell.execute_reply.started":"2023-07-21T22:33:19.396088Z","shell.execute_reply":"2023-07-21T22:33:19.396115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}