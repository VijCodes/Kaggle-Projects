{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Importing Modules","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import LayerNormalization,Input\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:04.210000Z","iopub.execute_input":"2023-07-28T06:02:04.210487Z","iopub.status.idle":"2023-07-28T06:02:26.166845Z","shell.execute_reply.started":"2023-07-28T06:02:04.210460Z","shell.execute_reply":"2023-07-28T06:02:26.165891Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/segment-anything-zip-file/segment-anything\")\nsys.path.append(\"/kaggle/input/smpmod/segmentation_models.pytorch/segmentation_models_pytorch\")\nimport losses as ls\nimport segment_anything as sam","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:26.168955Z","iopub.execute_input":"2023-07-28T06:02:26.169934Z","iopub.status.idle":"2023-07-28T06:02:26.715153Z","shell.execute_reply.started":"2023-07-28T06:02:26.169898Z","shell.execute_reply":"2023-07-28T06:02:26.714108Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#validation_meta = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation_metadata.json')\n#train_metadata = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/train_metadata.json')\nsample_submission = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:26.716463Z","iopub.execute_input":"2023-07-28T06:02:26.716883Z","iopub.status.idle":"2023-07-28T06:02:26.735878Z","shell.execute_reply.started":"2023-07-28T06:02:26.716849Z","shell.execute_reply":"2023-07-28T06:02:26.734976Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Configuration","metadata":{}},{"cell_type":"code","source":"config = {\n    'batch_size': 16,\n    'learning_rate': 0.001,\n    'n_epochs': 10,\n    'loss_smooth':1.0\n    # Add any other hyperparameters you want to track\n}\n\n# Use the hyperparameters from the config dictionary\nbatch_size = config['batch_size']\nlearning_rate = config['learning_rate']\nn_epochs = config['n_epochs']\nseq_len ,channels ,height ,width =  8 ,3 ,256 ,256","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:26.739764Z","iopub.execute_input":"2023-07-28T06:02:26.740038Z","iopub.status.idle":"2023-07-28T06:02:26.747685Z","shell.execute_reply.started":"2023-07-28T06:02:26.740015Z","shell.execute_reply":"2023-07-28T06:02:26.746707Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Setting up lazy-loading and preprocessing functions","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\" face=\"verdana\">\n\n| **Band Number** | **Description** | **Additional Details** |\n| --- | --- | --- |\n| **Band 8** | \"Upper-Level Tropospheric Water Vapor\" Band | This band is helpful in tracking upper-level atmospheric moisture and jet stream winds, aiding in the forecasting of severe weather events and heavy rainfall. |\n| **Band 9** | \"Mid-Level Tropospheric Water Vapor\" Band | This band captures images of mid-tropospheric moisture and atmospheric motion, valuable for identifying features such as tropical cyclones and thunderstorms. |\n| **Band 10** | \"Lower-level Water Vapor\" Band | The lower-level water vapor band assists in identifying low-level moisture content, aiding in the prediction of fog, frost, and low clouds. |\n| **Band 11** | \"Cloud-Top Phase\" Band | This band helps to determine cloud phases (water, mixed, ice) and heights, crucial for aviation safety and general weather prediction. |\n| **Band 12** | \"Ozone Band\" | The ozone band detects ozone concentration in the atmosphere, offering insights into ozone layer health and aiding in the prediction of UV index and air quality. |\n| **Band 13** | \"Clean\" IR Longwave Window Band | Primarily used for detection of clouds at all levels, sea surface temperature, and rainfall. |\n| **Band 14** | IR Longwave Window Band | This band is used for surface and cloud top temperature estimates, identifying cloud types and cloud motion. |\n| **Band 15** | \"Dirty\" Longwave Window Band | Used for estimation of lower-tropospheric water vapor, volcanic ash detection, and for improved rainfall estimation. |\n| **Band 16** | \"CO2\" Longwave Infrared | This band aids in the estimation of cloud height and temperature, especially for high-level clouds. |\n\n</font>","metadata":{}},{"cell_type":"code","source":"class ContrailsData(Dataset):\n    def __init__(self, data_paths, train_validation):\n        # Initialize your data, download, etc.\n        self.data_paths = data_paths\n        self.train_validation = train_validation\n    \n    def __len__(self):\n        return len(self.data_paths['path'])\n\n    def __getitem__(self, index):\n        obs_path = self.data_paths.iloc[index]['path']\n        data = np.load(obs_path+'/data.npy')\n        labels = np.load(obs_path+'/labels.npy')\n        labels = labels.transpose(2, 0, 1)\n        labels = torch.from_numpy(labels)\n        labels = labels.float()\n        return data, labels\n\n\ndef get_data_paths(root_dirs):\n    all_dirs = []\n    for root_dir in root_dirs:\n        for path in os.listdir(root_dir):\n            full_path = os.path.join(root_dir, path)\n            if os.path.isdir(full_path):\n                all_dirs.append(full_path)\n    return pd.DataFrame(all_dirs, columns=['path'])\n\n\nroot_dirs_train = ['/kaggle/input/contrails-data-part1','/kaggle/input/contrails-data-part2','/kaggle/input/contrails-data-part3',\n                   '/kaggle/input/contrails-data-part4','/kaggle/input/contrails-data-part5','/kaggle/input/contrails-data-part6',\n                   '/kaggle/input/contrails-data-part7','/kaggle/input/contrails-data-part8']\n\nroot_dirs_validation = ['/kaggle/input/contrails-validation']\n\ntrain_data_paths = get_data_paths(root_dirs_train)\nvalidation_data_paths = get_data_paths(root_dirs_validation)\n\n# Prepare your data loaders\ntrain_dataset = ContrailsData(train_data_paths, 'train')\nvalidation_dataset = ContrailsData(validation_data_paths, 'validation')\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:26.749064Z","iopub.execute_input":"2023-07-28T06:02:26.749495Z","iopub.status.idle":"2023-07-28T06:02:54.263555Z","shell.execute_reply.started":"2023-07-28T06:02:26.749452Z","shell.execute_reply":"2023-07-28T06:02:54.262451Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Dice Coefficient","metadata":{}},{"cell_type":"code","source":"class Dice(nn.Module):\n    def __init__(self, use_sigmoid=True):\n        super(Dice, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.use_sigmoid = use_sigmoid\n\n    def forward(self, inputs, targets, smooth=1):\n        if self.use_sigmoid:\n            inputs = self.sigmoid(inputs)       \n        \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return dice\n    \ndice = Dice()\n\nclass DiceThresholdTester:\n    \n    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n        self.model = model\n        self.data_loader = data_loader\n        self.cumulative_mask_pred = []\n        self.cumulative_mask_true = []\n        \n    def precalculate_prediction(self) -> None:\n        sigmoid = nn.Sigmoid()\n        for images, mask_true in self.data_loader:\n            if torch.cuda.is_available():\n                images = images.cuda()\n\n            mask_pred = sigmoid(model.forward(images))\n\n            self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n            self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n            \n        self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n        self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n\n        self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n        self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n    \n    def test_threshold(self, threshold: float) -> float:\n        _dice = Dice(use_sigmoid=False)\n        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n        return _dice(self.cumulative_mask_true, after_threshold).item()\n    \n#dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:54.265093Z","iopub.execute_input":"2023-07-28T06:02:54.265849Z","iopub.status.idle":"2023-07-28T06:02:54.281195Z","shell.execute_reply.started":"2023-07-28T06:02:54.265812Z","shell.execute_reply":"2023-07-28T06:02:54.280288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Models\n\n*  SimpleConv2D\n*  UNet\n*  Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"class SimpleConv2D(nn.Module):\n    def __init__(self):\n        super(SimpleConv2D, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv_final = nn.Conv2d(256, 1, kernel_size=1, stride=1)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.conv_final(x)\n        \n        # Add a Sigmoid activation function as the last layer\n        #x = torch.sigmoid(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:54.283005Z","iopub.execute_input":"2023-07-28T06:02:54.283396Z","iopub.status.idle":"2023-07-28T06:02:54.300373Z","shell.execute_reply.started":"2023-07-28T06:02:54.283362Z","shell.execute_reply":"2023-07-28T06:02:54.299338Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n        \n        self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        \n        self.conv6 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n        self.conv7 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv8 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv9 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv_final = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self,x):\n        x1 = F.relu(self.conv1(x))\n        x = F.max_pool2d(x1,kernel_size=(2))\n        \n        x2 = F.relu(self.conv2(x))\n        x = F.max_pool2d(x2,kernel_size=(2))\n        \n        x3 = F.relu(self.conv3(x))\n        x = F.max_pool2d(x3,kernel_size=(2))\n        \n        x4 = F.relu(self.conv4(x))\n        x = F.max_pool2d(x4,kernel_size=(2))\n        \n        x5 = F.relu(self.conv5(x))\n        \n        x6 = torch.cat([x4,self.upconv5(x5)],dim=-3)\n        x6 = F.relu(self.conv6(x6))\n        \n        x7 = torch.cat([x3,self.upconv4(x6)],dim=-3)\n        x7 = F.relu(self.conv7(x7))\n        \n        x8 = torch.cat([x2,self.upconv3(x7)],dim=-3)\n        x8 = F.relu(self.conv8(x8))\n        \n        x9 = torch.cat([x1,self.upconv2(x8)],dim=-3)\n        x9 = F.relu(self.conv9(x9))\n        \n        out=self.conv_final(x9)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:54.302015Z","iopub.execute_input":"2023-07-28T06:02:54.302742Z","iopub.status.idle":"2023-07-28T06:02:54.320372Z","shell.execute_reply.started":"2023-07-28T06:02:54.302709Z","shell.execute_reply":"2023-07-28T06:02:54.319353Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn\nimport random\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nbatch_size, seq_len, channels, height, width = 16,8,3,256,256\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        \n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n        self.bn1 = nn.BatchNorm2d(outputChannel)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outputChannel, outputChannel)\n        self.bn2 = nn.BatchNorm2d(outputChannel)\n        self.downsample = downsample\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        caOutput = self.ca(out)\n        out = caOutput * out\n        saOutput = self.sa(out)\n        out = saOutput * out\n        return out, saOutput\n\nclass BasicDownSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        return x\n    \nclass FrameAttention(nn.Module):\n    def __init__(self, heads, key_dim, embed_dim):\n        super().__init__()\n        self.heads = heads\n        self.key_dim = key_dim\n        self.embed_dim = embed_dim\n        self.Linear1 = nn.Linear(self.key_dim, self.embed_dim)\n        self.MultiHead = nn.MultiheadAttention(self.embed_dim, self.heads)\n        self.LayerNorm = nn.LayerNorm(self.embed_dim, eps=1e-6)\n        self.Linear2 = nn.Linear(self.embed_dim, self.key_dim)\n\n    def forward(self, x):\n        y = x[:, 3,:,:,:]\n        x = x.view(x.shape[0], x.shape[1], -1).transpose(0, 1)\n        x = self.Linear1(x)\n        attn_output, _ = self.MultiHead(x, x, x)\n        x = x + attn_output\n        x = self.LayerNorm(x)\n        x = self.Linear2(x)\n        x = x.transpose(0, 1).reshape(batch_size,-1,height ,width)\n        x = torch.cat((y, x), dim=1)\n    \n        return x\n    \n\nclass DownSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass BasicUpSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        return x\n    \nclass UpSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass UNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = BasicDownSample(inputChannel, 32)\n    self.downsample2 = BasicDownSample(32, 64)\n    self.downsample3 = BasicDownSample(64, 128)\n    self.downsample4 = BasicDownSample(128, 256)\n    self.downsample5 = BasicDownSample(256, 512)\n\n    self.upsample1 = BasicUpSample(512, 256)\n    self.upsample2 = BasicUpSample(512, 128)\n    self.upsample3 = BasicUpSample(256, 64)\n    self.upsample4 = BasicUpSample(128, 32)\n    self.upsample5 = BasicUpSample(64, 32)\n    self.classification = self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128 = self.downsample1(x)\n    scale64 = self.downsample2(scale128)\n    scale32 = self.downsample3(scale64)\n    scale16 = self.downsample4(scale32)\n    scale8 = self.downsample5(scale16)\n    upscale16 = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32 = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64 = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128 = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256 = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass AttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128, sa128down = self.downsample1(x)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass ResidualAttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.FrameAttention = FrameAttention(100 ,channels*height*width ,100)\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.residualBlock1 = ResidualBlock(512, 512)\n    self.residualBlock2 = ResidualBlock(512, 512)\n    self.residualBlock3 = ResidualBlock(512, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    FrameAtt = self.FrameAttention(x)\n    scale128, sa128down = self.downsample1(FrameAtt)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    scale8, sa8down = self.residualBlock1(scale8)\n    scale8, sa8down = self.residualBlock2(scale8)\n    scale8, sa8down = self.residualBlock3(scale8)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:54.321788Z","iopub.execute_input":"2023-07-28T06:02:54.322352Z","iopub.status.idle":"2023-07-28T06:02:54.394082Z","shell.execute_reply.started":"2023-07-28T06:02:54.322322Z","shell.execute_reply":"2023-07-28T06:02:54.393135Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Building New Layers","metadata":{}},{"cell_type":"markdown","source":"#### Training\n\n*  Had to decrease batch size and empty cache because of memory issues. \n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n# Define the device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize your model\n#model = SimpleConv2D().to(device)\n\n#model  = UNet().to(device)\n\ndata,labels = next(iter(train_dataloader))\n\nmodel  = ResidualAttentionUNet(inputChannel = 27 , outputChannel = 1).to(device)\n\n# Define a list of possible locations where the saved model state might be stored\nlocations = ['ResAttUnet2.pth', '/kaggle/input/resattunet-model-state/ResAttUnet.pth']\n\n# Iterate over the list of locations\nfor location in locations:\n    try:\n        # Try to load the model state from the current location\n        model.load_state_dict(torch.load(location))\n        # If loading succeeds, break out of the loop\n        break\n    except: \n        # If loading fails, continue to the next location\n        continue\nelse:\n    # If all locations fail, print an error message\n    print(\"Could not load appropriate model state from any location. Continue with training brand new model\")\n    \n\n# Define loss function\n#criterion = nn.BCEWithLogitsLoss()\n\ncriterion = ls.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# Training loop\nimport time\nfrom datetime import datetime, timedelta\n\n# Training loop\nfor epoch in range(n_epochs):\n    model.train()\n    running_loss = 0\n    batches = 0\n    start_time = time.time()\n    for images, labels in train_dataloader:\n        torch.cuda.empty_cache()\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        try: \n            outputs = model(images)\n        except: \n            print(f'image shape when model failed{images.shape}')\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        batches += 1\n        if batches % 100 == 0:\n            elapsed_time = time.time() - start_time\n            time_per_batch = elapsed_time / batches\n            remaining_batches = len(train_dataloader) - batches\n            remaining_time = remaining_batches * time_per_batch\n            total_remaining_time = remaining_time + (n_epochs - epoch - 1) * len(train_dataloader) * time_per_batch\n            current_time = datetime.now()\n            epoch_completion_time = current_time + timedelta(seconds=remaining_time)\n            all_epochs_completion_time = current_time + timedelta(seconds=total_remaining_time)\n            print(f'batches ran: {batches}, estimated epoch completion time: {epoch_completion_time.strftime(\"%Y-%m-%d %H:%M:%S\")}, estimated all epochs completion time: {all_epochs_completion_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    print('Train Loss: {:.4f}'.format(epoch_loss))\n    torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:02:54.399587Z","iopub.execute_input":"2023-07-28T06:02:54.399990Z","iopub.status.idle":"2023-07-28T06:09:31.935905Z","shell.execute_reply.started":"2023-07-28T06:02:54.399958Z","shell.execute_reply":"2023-07-28T06:09:31.934596Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Could not load appropriate model state from any location. Continue with training brand new model\nbatches ran: 100, estimated epoch completion time: 2023-07-28 06:41:21, estimated all epochs completion time: 2023-07-28 12:25:44\nbatches ran: 200, estimated epoch completion time: 2023-07-28 06:40:10, estimated all epochs completion time: 2023-07-28 12:13:49\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     62\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 63\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batches \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"locations = ['ResAttUnet2.pth', '/kaggle/input/resattunet-model-state/ResAttUnet.pth']\n\n# Iterate over the list of locations\nfor location in locations:\n    try:\n        # Try to load the model state from the current location\n        model.load_state_dict(torch.load(location))\n        print('latest_model')\n        # If loading succeeds, break out of the loop\n        break\n    except: \n        # If loading fails, continue to the next location\n        continue\nelse:\n    # If all locations fail, print an error message\n    print(\"Could not load appropriate model state from any location. Continue with training brand new model\")","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.937037Z","iopub.status.idle":"2023-07-28T06:09:31.938075Z","shell.execute_reply.started":"2023-07-28T06:09:31.937835Z","shell.execute_reply":"2023-07-28T06:09:31.937858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfile_path = 'ResAttUnet2.pth'  # replace with your file path\nsize_in_bytes = os.path.getsize(file_path)\n\n# convert size to MB\nsize_in_mb = size_in_bytes / (1024 * 1024)\n\nprint(f'The size of the file is {size_in_mb} MB')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.939234Z","iopub.status.idle":"2023-07-28T06:09:31.940106Z","shell.execute_reply.started":"2023-07-28T06:09:31.939853Z","shell.execute_reply":"2023-07-28T06:09:31.939877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model  = ResidualAttentionUNet(inputChannel = 27 , outputChannel = 1).to(device)\nmodel.load_state_dict(torch.load('ResAttUnet2.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.941382Z","iopub.status.idle":"2023-07-28T06:09:31.942309Z","shell.execute_reply.started":"2023-07-28T06:09:31.942075Z","shell.execute_reply":"2023-07-28T06:09:31.942097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Validation loop\n    model.FrameAttention.batch_size = 16\n    model.eval()\n    print(f\"Evaluating: {epoch}\")\n    with torch.no_grad():\n        running_loss = 0\n        for images, labels in validation_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            try: outputs = model(images)\n            except: continue\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(validation_dataloader.dataset)\n        print('Validation Loss: {:.4f}'.format(epoch_loss))\n        dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.948825Z","iopub.status.idle":"2023-07-28T06:09:31.949265Z","shell.execute_reply.started":"2023-07-28T06:09:31.949039Z","shell.execute_reply":"2023-07-28T06:09:31.949060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Saving the current state of the model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.950603Z","iopub.status.idle":"2023-07-28T06:09:31.951363Z","shell.execute_reply.started":"2023-07-28T06:09:31.951127Z","shell.execute_reply":"2023-07-28T06:09:31.951148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    running_loss = 0\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(validation_dataloader.dataset)\n    print('Validation Loss: {:.4f}'.format(epoch_loss))\ndice_threshold_tester.test_threshold(0.03)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.953056Z","iopub.status.idle":"2023-07-28T06:09:31.953805Z","shell.execute_reply.started":"2023-07-28T06:09:31.953550Z","shell.execute_reply":"2023-07-28T06:09:31.953571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_threshold_tester.test_threshold(0.03)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.955163Z","iopub.status.idle":"2023-07-28T06:09:31.955922Z","shell.execute_reply.started":"2023-07-28T06:09:31.955689Z","shell.execute_reply":"2023-07-28T06:09:31.955711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Optimal Dice Coefficient","metadata":{}},{"cell_type":"code","source":"dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)\ndice_threshold_tester.precalculate_prediction()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.957271Z","iopub.status.idle":"2023-07-28T06:09:31.958019Z","shell.execute_reply.started":"2023-07-28T06:09:31.957765Z","shell.execute_reply":"2023-07-28T06:09:31.957787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_to_test = [round(x * 0.001, 2) for x in range(200)]\n\noptim_threshold = 0.975\nbest_dice_score = -1\n\nthresholds = []\ndice_scores = []\n\nfor t in thresholds_to_test:\n    dice_score = dice_threshold_tester.test_threshold(t)\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        optim_threshold = t\n    \n    thresholds.append(t)\n    dice_scores.append(dice_score)\n    \nprint(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\ndf_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.959391Z","iopub.status.idle":"2023-07-28T06:09:31.960156Z","shell.execute_reply.started":"2023-07-28T06:09:31.959909Z","shell.execute_reply":"2023-07-28T06:09:31.959931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\nplt.axhline(y=best_dice_score, color='green')\nplt.axvline(x=optim_threshold, color='green')\nplt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\nplt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\nplt.ylim(bottom=0)\nplt.title('Threshold vs Dice Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-28T06:09:31.961496Z","iopub.status.idle":"2023-07-28T06:09:31.962259Z","shell.execute_reply.started":"2023-07-28T06:09:31.962020Z","shell.execute_reply":"2023-07-28T06:09:31.962042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}