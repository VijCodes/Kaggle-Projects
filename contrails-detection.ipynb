{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Importing Modules","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom tensorflow.keras import layers, Model, optimizers\nfrom tensorflow.keras.layers import LayerNormalization,Input","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:20:06.913709Z","iopub.execute_input":"2023-07-28T05:20:06.914236Z","iopub.status.idle":"2023-07-28T05:20:18.692711Z","shell.execute_reply.started":"2023-07-28T05:20:06.914206Z","shell.execute_reply":"2023-07-28T05:20:18.691861Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/segment-anything-zip-file/segment-anything\")\nsys.path.append(\"/kaggle/input/smpmod/segmentation_models.pytorch/segmentation_models_pytorch\")\nimport losses as ls\nimport segment_anything as sam","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:20:58.992400Z","iopub.execute_input":"2023-07-28T05:20:58.993002Z","iopub.status.idle":"2023-07-28T05:20:58.999133Z","shell.execute_reply.started":"2023-07-28T05:20:58.992967Z","shell.execute_reply":"2023-07-28T05:20:58.998032Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#validation_meta = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation_metadata.json')\n#train_metadata = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/train_metadata.json')\nsample_submission = pd.read_csv(r'/kaggle/input/google-research-identify-contrails-reduce-global-warming/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:21:04.490770Z","iopub.execute_input":"2023-07-28T05:21:04.491199Z","iopub.status.idle":"2023-07-28T05:21:04.498072Z","shell.execute_reply.started":"2023-07-28T05:21:04.491171Z","shell.execute_reply":"2023-07-28T05:21:04.497166Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Configuration","metadata":{}},{"cell_type":"code","source":"config = {\n    'batch_size': 16,\n    'learning_rate': 0.001,\n    'n_epochs': 10,\n    'loss_smooth':1.0\n    # Add any other hyperparameters you want to track\n}\n\n# Use the hyperparameters from the config dictionary\nbatch_size = config['batch_size']\nlearning_rate = config['learning_rate']\nn_epochs = config['n_epochs']\nseq_len ,channels ,height ,width =  8 ,3 ,256 ,256","metadata":{"execution":{"iopub.status.busy":"2023-07-28T05:20:19.120037Z","iopub.execute_input":"2023-07-28T05:20:19.120323Z","iopub.status.idle":"2023-07-28T05:20:19.126139Z","shell.execute_reply.started":"2023-07-28T05:20:19.120301Z","shell.execute_reply":"2023-07-28T05:20:19.125103Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Setting up lazy-loading and preprocessing functions","metadata":{}},{"cell_type":"markdown","source":"<font size=\"4\" face=\"verdana\">\n\n| **Band Number** | **Description** | **Additional Details** |\n| --- | --- | --- |\n| **Band 8** | \"Upper-Level Tropospheric Water Vapor\" Band | This band is helpful in tracking upper-level atmospheric moisture and jet stream winds, aiding in the forecasting of severe weather events and heavy rainfall. |\n| **Band 9** | \"Mid-Level Tropospheric Water Vapor\" Band | This band captures images of mid-tropospheric moisture and atmospheric motion, valuable for identifying features such as tropical cyclones and thunderstorms. |\n| **Band 10** | \"Lower-level Water Vapor\" Band | The lower-level water vapor band assists in identifying low-level moisture content, aiding in the prediction of fog, frost, and low clouds. |\n| **Band 11** | \"Cloud-Top Phase\" Band | This band helps to determine cloud phases (water, mixed, ice) and heights, crucial for aviation safety and general weather prediction. |\n| **Band 12** | \"Ozone Band\" | The ozone band detects ozone concentration in the atmosphere, offering insights into ozone layer health and aiding in the prediction of UV index and air quality. |\n| **Band 13** | \"Clean\" IR Longwave Window Band | Primarily used for detection of clouds at all levels, sea surface temperature, and rainfall. |\n| **Band 14** | IR Longwave Window Band | This band is used for surface and cloud top temperature estimates, identifying cloud types and cloud motion. |\n| **Band 15** | \"Dirty\" Longwave Window Band | Used for estimation of lower-tropospheric water vapor, volcanic ash detection, and for improved rainfall estimation. |\n| **Band 16** | \"CO2\" Longwave Infrared | This band aids in the estimation of cloud height and temperature, especially for high-level clouds. |\n\n</font>","metadata":{}},{"cell_type":"code","source":"class ContrailsData(Dataset):\n    def __init__(self, data_paths, preprocess_func, train_validation):\n        # Initialize your data, download, etc.\n        self.data_paths = data_paths\n        self.preprocess_func = preprocess_func\n        self.train_validation = train_validation\n    \n    def __len__(self):\n        return len(self.data_paths['path'])\n\n    def __getitem__(self, index):\n        obs_path = self.data_paths.iloc[index]['path']\n        bands_data, labels = getdata(obs_path)\n        data = self.preprocess_func(bands_data)\n        labels = labels.transpose(2, 0, 1)\n        labels = torch.from_numpy(labels)\n        labels = labels.float()\n        return data, labels\n\n\ndef get_data_paths(root_dir):\n    all_dirs = []\n    for path in os.listdir(root_dir):\n        full_path = os.path.join(root_dir, path)\n        if os.path.isdir(full_path):\n            all_dirs.append(full_path)\n    return pd.DataFrame(all_dirs, columns=['path'])\n\nroot_dir_train = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/train'\nroot_dir_validation = '/kaggle/input/google-research-identify-contrails-reduce-global-warming/validation'\n\ntrain_data_paths = get_data_paths(root_dir_train)\nvalidation_data_paths = get_data_paths(root_dir_validation)\n\n# Prepare your data loaders\ntrain_dataset = ContrailsData(train_data_paths, preprocess_func, 'train')\nvalidation_dataset = ContrailsData(validation_data_paths, preprocess_func, 'validation')\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:16:44.247632Z","iopub.execute_input":"2023-07-25T02:16:44.248101Z","iopub.status.idle":"2023-07-25T02:16:52.560658Z","shell.execute_reply.started":"2023-07-25T02:16:44.248067Z","shell.execute_reply":"2023-07-25T02:16:52.559847Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Dice Coefficient","metadata":{}},{"cell_type":"code","source":"class Dice(nn.Module):\n    def __init__(self, use_sigmoid=True):\n        super(Dice, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.use_sigmoid = use_sigmoid\n\n    def forward(self, inputs, targets, smooth=1):\n        if self.use_sigmoid:\n            inputs = self.sigmoid(inputs)       \n        \n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()\n        dice = (2.0 *intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return dice\n    \ndice = Dice()\n\nclass DiceThresholdTester:\n    \n    def __init__(self, model: nn.Module, data_loader: torch.utils.data.DataLoader):\n        self.model = model\n        self.data_loader = data_loader\n        self.cumulative_mask_pred = []\n        self.cumulative_mask_true = []\n        \n    def precalculate_prediction(self) -> None:\n        sigmoid = nn.Sigmoid()\n        for images, mask_true in self.data_loader:\n            if torch.cuda.is_available():\n                images = images.cuda()\n\n            mask_pred = sigmoid(model.forward(images))\n\n            self.cumulative_mask_pred.append(mask_pred.cpu().detach().numpy())\n            self.cumulative_mask_true.append(mask_true.cpu().detach().numpy())\n            \n        self.cumulative_mask_pred = np.concatenate(self.cumulative_mask_pred, axis=0)\n        self.cumulative_mask_true = np.concatenate(self.cumulative_mask_true, axis=0)\n\n        self.cumulative_mask_pred = torch.flatten(torch.from_numpy(self.cumulative_mask_pred))\n        self.cumulative_mask_true = torch.flatten(torch.from_numpy(self.cumulative_mask_true))\n    \n    def test_threshold(self, threshold: float) -> float:\n        _dice = Dice(use_sigmoid=False)\n        after_threshold = np.zeros(self.cumulative_mask_pred.shape)\n        after_threshold[self.cumulative_mask_pred[:] > threshold] = 1\n        after_threshold[self.cumulative_mask_pred[:] < threshold] = 0\n        after_threshold = torch.flatten(torch.from_numpy(after_threshold))\n        return _dice(self.cumulative_mask_true, after_threshold).item()\n    \n#dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:22:56.348159Z","iopub.execute_input":"2023-07-23T03:22:56.348551Z","iopub.status.idle":"2023-07-23T03:22:56.365356Z","shell.execute_reply.started":"2023-07-23T03:22:56.348521Z","shell.execute_reply":"2023-07-23T03:22:56.364238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Defining Models\n\n*  SimpleConv2D\n*  UNet\n*  Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"class SimpleConv2D(nn.Module):\n    def __init__(self):\n        super(SimpleConv2D, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv_final = nn.Conv2d(256, 1, kernel_size=1, stride=1)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.conv_final(x)\n        \n        # Add a Sigmoid activation function as the last layer\n        #x = torch.sigmoid(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:22:56.367630Z","iopub.execute_input":"2023-07-23T03:22:56.368097Z","iopub.status.idle":"2023-07-23T03:22:56.384243Z","shell.execute_reply.started":"2023-07-23T03:22:56.368057Z","shell.execute_reply":"2023-07-23T03:22:56.382957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1)\n        \n        self.upconv5 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        \n        self.conv6 = nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1)\n        self.conv7 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv8 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        self.conv9 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n        \n        self.conv_final = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self,x):\n        x1 = F.relu(self.conv1(x))\n        x = F.max_pool2d(x1,kernel_size=(2))\n        \n        x2 = F.relu(self.conv2(x))\n        x = F.max_pool2d(x2,kernel_size=(2))\n        \n        x3 = F.relu(self.conv3(x))\n        x = F.max_pool2d(x3,kernel_size=(2))\n        \n        x4 = F.relu(self.conv4(x))\n        x = F.max_pool2d(x4,kernel_size=(2))\n        \n        x5 = F.relu(self.conv5(x))\n        \n        x6 = torch.cat([x4,self.upconv5(x5)],dim=-3)\n        x6 = F.relu(self.conv6(x6))\n        \n        x7 = torch.cat([x3,self.upconv4(x6)],dim=-3)\n        x7 = F.relu(self.conv7(x7))\n        \n        x8 = torch.cat([x2,self.upconv3(x7)],dim=-3)\n        x8 = F.relu(self.conv8(x8))\n        \n        x9 = torch.cat([x1,self.upconv2(x8)],dim=-3)\n        x9 = F.relu(self.conv9(x9))\n        \n        out=self.conv_final(x9)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:22:56.386187Z","iopub.execute_input":"2023-07-23T03:22:56.386777Z","iopub.status.idle":"2023-07-23T03:22:56.408649Z","shell.execute_reply.started":"2023-07-23T03:22:56.386745Z","shell.execute_reply":"2023-07-23T03:22:56.407475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Residual Attention Unet","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn\nimport random\n\nrandom.seed(0)\nnp.random.seed(0)\ntorch.manual_seed(0)\nbatch_size, seq_len, channels, height, width = 16,8,3,256,256\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n           \n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        \n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n        self.bn1 = nn.BatchNorm2d(outputChannel)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outputChannel, outputChannel)\n        self.bn2 = nn.BatchNorm2d(outputChannel)\n        self.downsample = downsample\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        caOutput = self.ca(out)\n        out = caOutput * out\n        saOutput = self.sa(out)\n        out = saOutput * out\n        return out, saOutput\n\nclass BasicDownSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        return x\n    \nclass FrameAttention(nn.Module):\n    def __init__(self, heads, key_dim, embed_dim):\n        super().__init__()\n        self.heads = heads\n        self.key_dim = key_dim\n        self.embed_dim = embed_dim\n        self.Linear1 = nn.Linear(self.key_dim, self.embed_dim)\n        self.MultiHead = nn.MultiheadAttention(self.embed_dim, self.heads)\n        self.LayerNorm = nn.LayerNorm(self.embed_dim, eps=1e-6)\n        self.Linear2 = nn.Linear(self.embed_dim, self.key_dim)\n\n    def forward(self, x):\n        y = x[:, 3,:,:,:]\n        x = x.view(x.shape[0], x.shape[1], -1).transpose(0, 1)\n        x = self.Linear1(x)\n        attn_output, _ = self.MultiHead(x, x, x)\n        x = x + attn_output\n        x = self.LayerNorm(x)\n        x = self.Linear2(x)\n        x = x.transpose(0, 1).reshape(batch_size,-1,height ,width)\n        x = torch.cat((y, x), dim=1)\n    \n        return x\n    \n\nclass DownSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass BasicUpSample(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        return x\n    \nclass UpSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass UNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = BasicDownSample(inputChannel, 32)\n    self.downsample2 = BasicDownSample(32, 64)\n    self.downsample3 = BasicDownSample(64, 128)\n    self.downsample4 = BasicDownSample(128, 256)\n    self.downsample5 = BasicDownSample(256, 512)\n\n    self.upsample1 = BasicUpSample(512, 256)\n    self.upsample2 = BasicUpSample(512, 128)\n    self.upsample3 = BasicUpSample(256, 64)\n    self.upsample4 = BasicUpSample(128, 32)\n    self.upsample5 = BasicUpSample(64, 32)\n    self.classification = self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128 = self.downsample1(x)\n    scale64 = self.downsample2(scale128)\n    scale32 = self.downsample3(scale64)\n    scale16 = self.downsample4(scale32)\n    scale8 = self.downsample5(scale16)\n    upscale16 = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32 = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64 = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128 = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256 = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass AttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128, sa128down = self.downsample1(x)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput\n\nclass ResidualAttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.FrameAttention = FrameAttention(100 ,channels*height*width ,100)\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.residualBlock1 = ResidualBlock(512, 512)\n    self.residualBlock2 = ResidualBlock(512, 512)\n    self.residualBlock3 = ResidualBlock(512, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    FrameAtt = self.FrameAttention(x)\n    scale128, sa128down = self.downsample1(FrameAtt)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    scale8, sa8down = self.residualBlock1(scale8)\n    scale8, sa8down = self.residualBlock2(scale8)\n    scale8, sa8down = self.residualBlock3(scale8)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:17:07.208751Z","iopub.execute_input":"2023-07-25T02:17:07.209184Z","iopub.status.idle":"2023-07-25T02:17:07.264572Z","shell.execute_reply.started":"2023-07-25T02:17:07.209154Z","shell.execute_reply":"2023-07-25T02:17:07.263384Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Building New Layers","metadata":{}},{"cell_type":"markdown","source":"#### Training\n\n*  Had to decrease batch size and empty cache because of memory issues. \n","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n# Define the device for training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize your model\n#model = SimpleConv2D().to(device)\n\n#model  = UNet().to(device)\n\ndata,labels = next(iter(train_dataloader))\n\nmodel  = ResidualAttentionUNet(inputChannel = 27 , outputChannel = 1).to(device)\n\n# Define a list of possible locations where the saved model state might be stored\nlocations = ['ResAttUnet2.pth', '/kaggle/input/resattunet-model-state/ResAttUnet.pth']\n\n# Iterate over the list of locations\nfor location in locations:\n    try:\n        # Try to load the model state from the current location\n        model.load_state_dict(torch.load(location))\n        # If loading succeeds, break out of the loop\n        break\n    except: \n        # If loading fails, continue to the next location\n        continue\nelse:\n    # If all locations fail, print an error message\n    print(\"Could not load appropriate model state from any location. Continue with training brand new model\")\n    \n\n# Define loss function\n#criterion = nn.BCEWithLogitsLoss()\n\ncriterion = ls.DiceLoss(mode=\"binary\", smooth=config[\"loss_smooth\"])\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n# Training loop\nimport time\nfrom datetime import datetime, timedelta\n\n# Training loop\nfor epoch in range(n_epochs):\n    model.train()\n    running_loss = 0\n    batches = 0\n    start_time = time.time()\n    for images, labels in train_dataloader:\n        torch.cuda.empty_cache()\n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        try: \n            outputs = model(images)\n        except: \n            print(f'image shape when model failed{images.shape}')\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * images.size(0)\n        batches += 1\n        if batches % 100 == 0:\n            elapsed_time = time.time() - start_time\n            time_per_batch = elapsed_time / batches\n            remaining_batches = len(train_dataloader) - batches\n            remaining_time = remaining_batches * time_per_batch\n            total_remaining_time = remaining_time + (n_epochs - epoch - 1) * len(train_dataloader) * time_per_batch\n            current_time = datetime.now()\n            epoch_completion_time = current_time + timedelta(seconds=remaining_time)\n            all_epochs_completion_time = current_time + timedelta(seconds=total_remaining_time)\n            print(f'batches ran: {batches}, estimated epoch completion time: {epoch_completion_time.strftime(\"%Y-%m-%d %H:%M:%S\")}, estimated all epochs completion time: {all_epochs_completion_time.strftime(\"%Y-%m-%d %H:%M:%S\")}')\n\n    epoch_loss = running_loss / len(train_dataloader.dataset)\n    print('Train Loss: {:.4f}'.format(epoch_loss))\n    torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:22:56.486067Z","iopub.execute_input":"2023-07-23T03:22:56.487189Z","iopub.status.idle":"2023-07-23T03:23:07.924843Z","shell.execute_reply.started":"2023-07-23T03:22:56.487141Z","shell.execute_reply":"2023-07-23T03:23:07.922011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor images, labels in train_dataloader:\n        images = images\n        labels = labels","metadata":{"execution":{"iopub.status.busy":"2023-07-25T02:19:08.070241Z","iopub.execute_input":"2023-07-25T02:19:08.070690Z","iopub.status.idle":"2023-07-25T02:19:13.767277Z","shell.execute_reply.started":"2023-07-25T02:19:08.070656Z","shell.execute_reply":"2023-07-25T02:19:13.765820Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      3\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mContrailsData.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     12\u001b[0m     obs_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_paths\u001b[38;5;241m.\u001b[39miloc[index][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m     bands_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mgetdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_func(bands_data)\n\u001b[1;32m     15\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mgetdata\u001b[0;34m(obs_path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         band_name \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# get the name of the band (excluding the .npy extension)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(obs_path, filename)  \u001b[38;5;66;03m# full path of the file\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         band_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load the band data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         bands_data[band_name] \u001b[38;5;241m=\u001b[39m band_data  \u001b[38;5;66;03m# store the band data in the dictionary\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Load the aggregated contrail markings as labels\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/format.py:790\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;66;03m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;66;03m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    803\u001b[0m         array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray(count, dtype\u001b[38;5;241m=\u001b[39mdtype)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"images.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-25T16:57:32.377264Z","iopub.execute_input":"2023-07-25T16:57:32.377694Z","iopub.status.idle":"2023-07-25T16:57:32.931662Z","shell.execute_reply.started":"2023-07-25T16:57:32.377656Z","shell.execute_reply":"2023-07-25T16:57:32.929894Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimages\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"],"ename":"NameError","evalue":"name 'images' is not defined","output_type":"error"}]},{"cell_type":"code","source":"locations = ['ResAttUnet2.pth', '/kaggle/input/resattunet-model-state/ResAttUnet.pth']\n\n# Iterate over the list of locations\nfor location in locations:\n    try:\n        # Try to load the model state from the current location\n        model.load_state_dict(torch.load(location))\n        print('latest_model')\n        # If loading succeeds, break out of the loop\n        break\n    except: \n        # If loading fails, continue to the next location\n        continue\nelse:\n    # If all locations fail, print an error message\n    print(\"Could not load appropriate model state from any location. Continue with training brand new model\")","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:57.408225Z","iopub.execute_input":"2023-07-23T03:23:57.408818Z","iopub.status.idle":"2023-07-23T03:23:57.721955Z","shell.execute_reply.started":"2023-07-23T03:23:57.408775Z","shell.execute_reply":"2023-07-23T03:23:57.720942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfile_path = 'ResAttUnet2.pth'  # replace with your file path\nsize_in_bytes = os.path.getsize(file_path)\n\n# convert size to MB\nsize_in_mb = size_in_bytes / (1024 * 1024)\n\nprint(f'The size of the file is {size_in_mb} MB')","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:28:13.439676Z","iopub.execute_input":"2023-07-23T03:28:13.440203Z","iopub.status.idle":"2023-07-23T03:28:13.447124Z","shell.execute_reply.started":"2023-07-23T03:28:13.440147Z","shell.execute_reply":"2023-07-23T03:28:13.446025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model  = ResidualAttentionUNet(inputChannel = 27 , outputChannel = 1).to(device)\nmodel.load_state_dict(torch.load('ResAttUnet2.pth'))","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.932573Z","iopub.status.idle":"2023-07-23T03:23:07.933004Z","shell.execute_reply.started":"2023-07-23T03:23:07.932809Z","shell.execute_reply":"2023-07-23T03:23:07.932828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Validation loop\n    model.FrameAttention.batch_size = 16\n    model.eval()\n    print(f\"Evaluating: {epoch}\")\n    with torch.no_grad():\n        running_loss = 0\n        for images, labels in validation_dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            try: outputs = model(images)\n            except: continue\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * images.size(0)\n\n        epoch_loss = running_loss / len(validation_dataloader.dataset)\n        print('Validation Loss: {:.4f}'.format(epoch_loss))\n        dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:29:22.627699Z","iopub.execute_input":"2023-07-23T03:29:22.628084Z","iopub.status.idle":"2023-07-23T03:36:03.246521Z","shell.execute_reply.started":"2023-07-23T03:29:22.628052Z","shell.execute_reply":"2023-07-23T03:36:03.245367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Saving the current state of the model","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'ResAttUnet2.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.937899Z","iopub.status.idle":"2023-07-23T03:23:07.938645Z","shell.execute_reply.started":"2023-07-23T03:23:07.938409Z","shell.execute_reply":"2023-07-23T03:23:07.938432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    running_loss = 0\n    for images, labels in validation_dataloader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item() * images.size(0)\n\n    epoch_loss = running_loss / len(validation_dataloader.dataset)\n    print('Validation Loss: {:.4f}'.format(epoch_loss))\ndice_threshold_tester.test_threshold(0.03)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.939939Z","iopub.status.idle":"2023-07-23T03:23:07.940712Z","shell.execute_reply.started":"2023-07-23T03:23:07.940460Z","shell.execute_reply":"2023-07-23T03:23:07.940483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_threshold_tester.test_threshold(0.03)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:39:59.644215Z","iopub.execute_input":"2023-07-23T03:39:59.644628Z","iopub.status.idle":"2023-07-23T03:39:59.766218Z","shell.execute_reply.started":"2023-07-23T03:39:59.644596Z","shell.execute_reply":"2023-07-23T03:39:59.763349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Optimal Dice Coefficient","metadata":{}},{"cell_type":"code","source":"dice_threshold_tester = DiceThresholdTester(model, validation_dataloader)\ndice_threshold_tester.precalculate_prediction()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.942066Z","iopub.status.idle":"2023-07-23T03:23:07.942840Z","shell.execute_reply.started":"2023-07-23T03:23:07.942586Z","shell.execute_reply":"2023-07-23T03:23:07.942610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_to_test = [round(x * 0.001, 2) for x in range(200)]\n\noptim_threshold = 0.975\nbest_dice_score = -1\n\nthresholds = []\ndice_scores = []\n\nfor t in thresholds_to_test:\n    dice_score = dice_threshold_tester.test_threshold(t)\n    if dice_score > best_dice_score:\n        best_dice_score = dice_score\n        optim_threshold = t\n    \n    thresholds.append(t)\n    dice_scores.append(dice_score)\n    \nprint(f'Best Threshold: {optim_threshold} with dice: {best_dice_score}')\ndf_threshold_data = pd.DataFrame({'Threshold': thresholds, 'Dice Score': dice_scores})","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.944676Z","iopub.status.idle":"2023-07-23T03:23:07.945452Z","shell.execute_reply.started":"2023-07-23T03:23:07.945186Z","shell.execute_reply":"2023-07-23T03:23:07.945220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.lineplot(data=df_threshold_data, x='Threshold', y='Dice Score')\nplt.axhline(y=best_dice_score, color='green')\nplt.axvline(x=optim_threshold, color='green')\nplt.text(-0.02, best_dice_score * 0.96, f'{best_dice_score:.3f}', va='center', ha='left', color='green')\nplt.text(optim_threshold - 0.01, 0.02, f'{optim_threshold}', va='center', ha='right', color='green')\nplt.ylim(bottom=0)\nplt.title('Threshold vs Dice Score')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-23T03:23:07.946832Z","iopub.status.idle":"2023-07-23T03:23:07.947593Z","shell.execute_reply.started":"2023-07-23T03:23:07.947358Z","shell.execute_reply":"2023-07-23T03:23:07.947380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}